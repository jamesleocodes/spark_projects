{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "file_path = \"/data/WA_Fn-UseC_-Telco-Customer-Churn.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/25 23:14:34 WARN Utils: Your hostname, mbp.local resolves to a loopback address: 127.0.0.1; using 10.20.5.195 instead (on interface en0)\n",
      "25/04/25 23:14:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/25 23:14:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TelcoCustomerChurnPrediction\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the Telco Customer Churn dataset\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "        print(\"Dataset loaded successfully.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Preprocessing\n",
    "def preprocess_data(df):\n",
    "    # Drop customerID as it's not needed for prediction\n",
    "    df = df.drop(\"customerID\")\n",
    "    \n",
    "    # Handle missing values (TotalCharges may have empty strings or nulls)\n",
    "    df = df.filter(col(\"TotalCharges\") != \" \")  # Remove rows with empty TotalCharges\n",
    "    df = df.withColumn(\"TotalCharges\", col(\"TotalCharges\").cast(\"float\"))\n",
    "    \n",
    "    # Define categorical and numerical columns\n",
    "    categorical_columns = [\n",
    "        \"gender\", \"SeniorCitizen\", \"Partner\", \"Dependents\", \"PhoneService\",\n",
    "        \"MultipleLines\", \"InternetService\", \"OnlineSecurity\", \"OnlineBackup\",\n",
    "        \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\",\n",
    "        \"Contract\", \"PaperlessBilling\", \"PaymentMethod\"\n",
    "    ]\n",
    "    numerical_columns = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "    \n",
    "    # Index categorical columns\n",
    "    indexed_columns = [f\"{col}_indexed\" for col in categorical_columns]\n",
    "    indexers = [\n",
    "        StringIndexer(inputCol=col, outputCol=f\"{col}_indexed\", handleInvalid=\"keep\")\n",
    "        for col in categorical_columns\n",
    "    ]\n",
    "    \n",
    "    for indexer in indexers:\n",
    "        df = indexer.fit(df).transform(df)\n",
    "    \n",
    "    # Index target variable (Churn)\n",
    "    df = StringIndexer(inputCol=\"Churn\", outputCol=\"label\").fit(df).transform(df)\n",
    "    \n",
    "    # Assemble features\n",
    "    feature_columns = numerical_columns + indexed_columns\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"raw_features\", handleInvalid=\"skip\")\n",
    "    df = assembler.transform(df)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "    df = scaler.fit(df).transform(df)\n",
    "    \n",
    "    return df, feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Main function\n",
    "def main():\n",
    "    # Path to the dataset (update this to your local path)\n",
    "    file_path = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "    \n",
    "    # Load data\n",
    "    df = load_data(file_path)\n",
    "    if df is None:\n",
    "        print(\"Exiting due to data loading error.\")\n",
    "        return\n",
    "    # Preprocess data\n",
    "    df_processed, feature_columns = preprocess_data(df)\n",
    "    \n",
    "    # Train-test split\n",
    "    train_df, test_df = df_processed.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    # Train Random Forest model\n",
    "    rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100, seed=42)\n",
    "    model = rf.fit(train_df)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.transform(test_df)\n",
    "    \n",
    "    # Evaluate model\n",
    "    evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "    evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "    evaluator_roc = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "    \n",
    "    accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "    precision = evaluator_precision.evaluate(predictions)\n",
    "    recall = evaluator_recall.evaluate(predictions)\n",
    "    roc_auc = evaluator_roc.evaluate(predictions)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': model.featureImportances.toArray()\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "    plt.title('Feature Importance in Telco Churn Prediction')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('telco_feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Save feature importance data\n",
    "    feature_importance.to_csv('telco_feature_importance.csv', index=False)\n",
    "    print(\"\\nFeature importance plot saved as 'telco_feature_importance.png'\")\n",
    "    print(\"Feature importance data saved as 'telco_feature_importance.csv'\")\n",
    "    \n",
    "    # Stop Spark session\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading dataset: [PATH_NOT_FOUND] Path does not exist: file:/Users/zaw/github_repos/Spark/WA_Fn-UseC_-Telco-Customer-Churn.csv.\n",
      "Exiting due to data loading error.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
